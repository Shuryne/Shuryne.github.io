# cs231n笔记

###### 课程：CS231n  


## 视觉的出现 
1. 视觉神经的出现，使得远古生物开始“看得见”世界的色彩，并与物种大爆发有密切联系 


## 图像分类 
1. 数据驱动 
1. k近邻：距离度量、纬度灾难 
1. 线性分类：模版匹配/高维空间的线性决策边界 


## 损失函数 
1. sigmoid 
    * 优点：模拟生物信号，非线性激活神经元 
    * 缺点 
        * 远端饱和 
        * 输出恒正，导致zigzag问题
1. tanh 
    * 优点：输出可正可负，不会导致zigzag问题 
    * 缺点：和sigmoid是线性关系，伸缩平移即可得到，同样存在远端饱和问题 
1. relu系列 
    * 优点：收敛快，不会导致梯度饱和问题 
    * 缺点：梯度回传一旦较大，即死亡，对lr比较敏感 
    * Leaky relu 
    * ELU 


## 数据归一化 
1. 归一化之后，损失函数对轻微扰动不很敏感，更容易学习 
1. BN：数据归一化之后，学习缩放参数平移参数，增加该层的可表达性 


## 参数初始化 
1. 太小：激活函数输出和梯度会趋于0，不利于学习 
1. 太大：激活函数会饱和，梯度趋于0，不利于学习 
1. 合适：Xavier / MSRA 
1. 网络越深，参数初始化越重要 


## 优化 
1. SGD 
    * Mini batch有噪声/随机性 
1. 动量 
    * 使得梯度有惯量，跳出局部最小/鞍点 
1. Adam 
    * 使用一阶动量和二阶动量进行梯度修正，使得梯度变化较大的方向步长较小，反之亦然 


## dropout 
1. 避免网络总是通过特征组合进行判断，训练网络能够通过零散的特征进行判断，一定程度上缓解过拟合 
1. 是单一模型的集成学习，对一群共享参数的子网络进行集成 
1. 预测时，激活函数输出需要乘上dropout概率值 
1. 和BN具有类似的正则化效果，都是倾向于在训练时加入随机性/噪声，但是dropout比BN更加可控，可以随时调整 


## CNN 
1. AlexNet 
1. VGG 
1. GoogleNet 
1. ResNet 

## RNN 
1. LSTM梯度 


## 目标检测与图像分割 
1. 去pooling 
1. 去卷积/转置卷积


