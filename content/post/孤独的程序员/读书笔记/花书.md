---
title: Deep Learning（花书）笔记
date: 2020-03-20
draft: false
categories:
- 无聊的幻想家
tags:
- 笔记

---



# Part 1 数学基础

## 线性代数

  * 线性相关/线性无关
  * F范数：矩阵的L2范数
  * 特征值分解（正定等概念）
  * 奇异值分解
  * 行列式：特征值乘积/体积
  * 伪逆
  * 例子：PCA
    * 最大重构性：重构误差最小化
    * 最大可分性：变换后协方差最大化
    * 两种推导等价

## 概率与信息论

  * 信息论
    * 互信息、熵、KL散度、交叉熵

## 数值计算

  * 溢出
    * 上溢出：减去最大值
  * 病态条件
    * 最大特征值/最小特征值较大时，求逆会有误差
  * 梯度优化
    * Jacobian 和 Hessian 矩阵
    * 最优步长：H矩阵最大特征值决定学习率量级
    * H矩阵的特征值：鞍点、极大、极小
    * 牛顿法
  * 约束优化
    * KKT条件
      * 广义 Lagrangian 的梯度为零
      * 所有关于 x 和 KKT 乘子的约束都满足
      * 不等式约束显示的 ‘‘互补松弛性’’
  * 例子：最小二乘

## 机器学习基础

  * 验证集：选择超参数/交叉验证
  * 最大似然估计
    * 等价于最小化KL散度
    * 高斯参数估计中，ML估计等价于MSE估计
  * 贝叶斯估计
    * 参数服从某种先验概率分布
    * 最大后验估计（MAP）
    * 权重衰减的ML估计等价于高斯先验的MAP
  * 监督学习
    * 逻辑回归、SVM、决策树
  * 无监督学习
    * PCA、K均值
    * 聚类问题有可能本身是病态的，聚类的性质很可能不能对应上现实性质
  * 流型学习：大部分区域都是无效的输入，有意义的输入只分布在包含少量数据点的子集构成的一组流形中，而学习函数的输出中，有意义的变化都沿着流形的方向或仅发生在我们切换到另一流形时。

  

# Part 2 深度神经网络

## 深度前馈网络

  * 隐藏单元
    * sigmoid：二项分布的
    * softmax单元：多项分布，强烈惩罚最活跃的不正确预测
    * Relu：梯度为1
    * maxout：每组取最大值输出
    * tanh：0附近近似单位函数
  * BP算法
  * 历史
    * 不变：BP、梯度下降
    * 改变：采用交叉熵损失函数代替均方误差损失、采用Relu替代sigmoid


## 正则化
  * 参数范数惩罚
    * L2：高斯先验、H特征值放缩
    * L1：拉普拉斯先验、稀疏解、特征选择
  * 数据集增强
  * 噪声
    * 输入加噪声
    * 权重加噪声：鼓励参数进入权重小扰动对输出相对影响较小的参数空间区域
    * 输出加噪声
      * 标签平滑：使用 softmax 函数和明确目标的最大似然学习可能永远不会收敛，能够防止模型追求确切概率而不影响模型学习正确分类
  * 多任务学习：共享底层参数
  * early stopping
    * 需要验证集
    * 正则化机制
      * 大曲率方向参数更早学习到，某种程度上相当于L2
      * 比权重衰减更有优势，能够自动确定正则化参数量，权重衰减需要调参
  * bagging
  * dropout
    * 集成大量深层神经网络的实用Bagging方法：计算方便、通用
    * Dropout强大的大部分原因来自施加到隐藏单元的掩码噪声：破坏提取的特征而不是原始值，让破坏过程充分利用该模型迄今获得的关于输入分布的所有知识
    * Dropout的另一个重要方面是噪声是乘性的：加性噪声容易被克服

## 优化

  * 挑战
    * 病态：梯度显著，但是二次项值大，导致目标函数下降慢，牛顿法可以解决
    * 高维空间，鞍点激增，无法使用牛顿法
    * 梯度消失与梯度爆炸
      * 很深的前馈网络，产生长期依赖，丧失先前信息
      * 循环网络反复乘同一矩阵，导致特征值消失/爆炸
      * 很深的前馈网络能很大程度避免梯度消失与梯度爆炸
  * SGD
  * 动量SGD
    * 解决 Hessian 矩阵的病态条件和随机梯度的方差
  * AdaGrad
  * RMSProp
  * Adam
  * 牛顿法
  * 优化策略
    * BN
    * 监督预训练 + finetune
    * FitNet：学生老师模型
    * 延拓法：代价函数序列，难度递升，克服局部最小
    * 课程学习：学习简单概念，再学习复杂概念


## 卷积神经网络
  * 卷积
    * 稀疏交互：核大小远小于输入大小，高效，减少计算量
    * 参数共享：一个核在不同位置都使用
    * 等变表示：平移等变，输入平移，输出也平移
  * 池化
    * 最大池化：局部/少量平移不变性
  * 无限强的先验
    * 卷积：卷积层函数只包含局部连接关系并且对平移具有等变性
    * 池化：每一个单元都具有对少量平移的不变性


## 循环/递归神经网络
  * 循环神经网络
    * 隐层到隐层：只能串行、性能高、效率低
    * 输出层到隐层：可并行化、易训练
  * 双向RNN：具有双向信息
  * Encoder-Decoder结构
  * 递归神经网络
  * 长期依赖
    * 梯度爆炸：梯度截断/随机梯度
    * LSTM
    * GRU
  * 解决梯度消失与梯度爆炸
    * BN/LN
    * 梯度截断/随机梯度

