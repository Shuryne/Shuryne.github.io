# 李宏毅机器学习

[课程笔记](https://datawhalechina.github.io/leeml-notes/#/)


## 误差从哪里来？

- **[理解`bias-variance tradeoff`](http://scott.fortmann-roe.com/docs/BiasVariance.html)**
- **[误差、偏差、方差的联系和区别](https://www.zhihu.com/question/27068705)**
- **[偏差和方差的理论推导](https://segmentfault.com/a/1190000016447144)**

问题1:  模型选择存在`bias-variance trade-off`，误差期望分解为三部分：随机噪声、偏差、方差，这个分解是建立在回归任务中并且误差采用均方误差的计算方式，如果换不同的任务、换不同的误差计算方式（如cross-entropy loss），结果会有所不同吗？

理解不正确，`bias-variance trade-off`一般是针对模型泛化来说的，指的是训练数据永远不可能穷尽真实世界数据的情况下，模型只能见到部分数据，在测试集上测试时会因为模型复杂度过低/过高呈现`underfit/overfit`的结果，和具体任务或者误差损失的计算方式无关。回归任务采用均方误差的损失计算方式能够数学化将误差期望显式分解成三部分，更加具体化将`trade-off`表现出来。分类任务也能做类似的分解，如链接1中写出了`KNN`的误差分解公示。


问题2: 模型初始化、先验知识概率分布假设大多使用高斯分布，是因为当二项分布中n趋于无穷时，整个分布趋于高斯分布，使用高斯分布暗含的前提假设是，当前事件/任务发生的原因是由无穷个伯努利分布小事件组成，但是现实世界中的很多事件并不是由无穷个小事件组成，而是简单的有穷个小事件即可实现，直接暴力采用高斯分布这样会不会不太妥当？


问题3: 是否能够将不同的基础概率分布当成组件，构建更加适合特定任务的概率分布？如何构建更加有效的概率分布？


1. 正则项与奥卡姆剃刀
	- 模型训练过程中，为了避免过拟合通常会加入正则项，限制参数大小，以达到缓和过拟合的目的，其背后的根本原因在于**奥卡姆剃刀**，更加具备泛化性的模型，其模型结构应该更加简单。在这里，**简单**对应的含义便是参数应该都比较小，从而模型能够更加平滑。奥卡姆剃刀经验性的成暗示我们当前的世界物理法则存在哲学（或者说经验哲学）上的真命题：我们对这个世界法则的复杂度感知稍显良好。这应该是人类大脑进化的结果，对于人类来说简单的定理法则，在大猩猩看来极其晦涩难懂。假设世界法则在理解/发现难度上的不同区分，那么奥卡姆剃刀便是一个自然的结果：随着人类大脑和工具的不断进步，人类从最基础的法则开始，不断发现、理解、运用，慢慢过渡到更加上层、复杂、艰深的知识外延，在这个过程中人脑是不断适应知识的复杂性的，并且不断的将复杂的法则内化成为简单的知识。
	- 举个例子，几十年前的普通人理解相对论和量子力学倍感吃力，得益于人类社会内部知识消化和传递的高效，在不久的将来普通人将能够轻松理解原本艰难难懂的理论。大部分的人会一直保持所理解和掌握的知识是在理解范围内的，也即是“简单的”，所以奥卡姆剃刀是一个自然的结果。



## 概率分类模型与逻辑回归

### 朴素贝叶斯
1. 朴素贝叶斯模型是一种基于贝叶斯定理的参数模型，“朴素”思想在于其强假设：各变量之间相互独立。这使得变量协方差矩阵只有对角线不为零，从而实现线性计算时间。
1. 模型常用于做分类任务。最大后验概率估计（MAP）是一种常用于估计朴素贝叶斯模型参数的方法，要注意区分。

### 逻辑回归
1. 逻辑回归是在线性回归的基础上增加零非线性激活函数，从而利用回归的方式实现分类的任务。
1. 逻辑回归使用交叉熵计算损失，交叉熵可以看作是计算两个伯努利分布有多接近。
