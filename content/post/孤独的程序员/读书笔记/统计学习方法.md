---
title: 统计学习方法笔记
date: 2020-03-18
draft: true
categories:
- 孤独的程序员
tags:

---

## 概论

  * 模型、策略、算法
  * 经验风险/结构风险最小化
  * 过拟合
    * 正则化：L1/L2
    * 交叉验证：划分验证集再不同复杂度模型中选择测试误差最小的
  * 生成模型
    * 能还原出P(X, Y)
    * 基本只有朴素贝叶斯、HMM
  * 判别模型
    * 直接学习P(Y|X)
    * SVM等大多数机器学习模型

L1和L2的区别联系是什么？

* L1对应拉普拉斯分布，L1产生稀疏特征，便于特征选择
* L2对应高斯分布，产生更多特征但是接近0
* 为什么用L2不用L1：L2计算方便、L2有唯一解，L1有多解 

ROC曲线是什么？AUC表示什么含义？

* ROC曲线为TP-FN图 
* AUC为ROC下的面积，表示分类器的分类能力，同时考虑对于正/负例的分类能力，能够规避样本不平衡带来的问题 
* https://www.zhihu.com/question/39840928
* https://blog.csdn.net/u013385925/article/details/80385873 

## 感知机

  * 损失函数：误分类点到超平面的总距离
  * 初值敏感、梯度下降更新参数
  * 线性可分时，迭代收敛：Novikoff定理


## k近邻
  * 三要素
    * 距离函数
    * k值大小：K越大，复杂度越小，交叉验证选择最佳K
    * 分类决策规则：多数表决
  * 实现：kd树


## 朴素贝叶斯法
  * 条件独立假设：分类特征在类确定情况下相互独立
  * 0-1损失期望风险最小化等价于后验概率最大化
  * 概论估计方法
    * 极大似然估计
    * 贝叶斯估计（拉普拉斯平滑）


## 决策树
  * ID3
    * 信息增益
    * 简单快速
  * C4.5
    * 信息增益比：能够避免选择属性值较多的属性
    * 能够处理连续值与缺失值
    * 剪枝：前后的损失函数决定是否剪枝
  * CART
    * Gini指数：纯度、简化运算
    * 分类：Gini指数最小化
    * 回归：平方损失最小化
    * 剪枝：子树序列中交叉验证选择最佳子树
    * 所得树一定为二叉树



## 逻辑回归与最大熵模型

逻辑回归

* 逻辑回归是对数几率线性模型
* 线性回归解决回归任务，逻辑回归解决分类任务，逻辑回归在线性回归的基础上加sigmoid函数用于平滑分类
* BCE Loss，梯度下降优化，更新参数值为真实值与预测值之间的差值 
* 此部分需要手推逻辑回归以及反向传播过程！

最大熵模型

  * 最大熵原理：所有可能的数据分布概率模型中，熵最大的模型时最好的

## SVM

  * 函数距离与几何距离
  * 完全线性可分：硬间隔
  * 近似线性可分：软间隔
  * 完全非线性可分：核方法
  * 算法：SMO
  * 合页损失
    * 线性支持向量机学习等价于最小化二阶范数正则化的合页函数
    * 合页损失不仅要分类正确，而且确信度足够高时损失才是0


## Boosting
  * AdaBoost：
    * 能适应弱分类器各自的训练误差率
    * 加法模型
    * 损失函数为指数损失
    * 前向分步算法
  * GBDT
    * 回归：平方误差损失函数，每次拟合残差
    * 分类：指数损失函数
    * 一般问题：梯度提升

* XGBOOST：
    * GBDT只利用了一阶梯度信息，在GBDT基础上对损失函数进行二阶展开，并加入正则项，拟合方向更准，速度更快
    * GBDT是一种算法，而XGBoost则是GBDT的系统性工程实现
    * XGBOOST相对于GBDT的优势
        * 加入正则，防止过拟合
        * 利用二阶信息，速度更快
        * 不仅支持CART分类器，也支持线性分类器
        * 其他性能方面的加速 
    * https://zhuanlan.zhihu.com/p/42740654
    * https://www.zhihu.com/question/41354392 

集成学习有哪些方法？有什么区别？

* Bagging： 不同数据集模型的ensemble
* Boosting： 相同数据集不同权重模型的ensemble
* 随机森林： 决策树的bagging版


## EM
  * E步：对对数似然求期望
  * M步：求最大化期望对数似然得到参数更新值
  * 收敛性：对数似然函数序列收敛，不保证收敛到最大值
  * 例子：三硬币、高斯混合模型


## HMM（隐马尔可夫）
  * 概率计算：前向后向算法
  * 学习模型参数：EM算法
  * 预测状态序列：维特比算法


## CRF（条件随机场）
  * 输出变量是无向图
  * 边特征、点特征









PCA怎么操作？LDA怎么操作？两者之间有什么区别和联系？

* PCA：无监督线性降维，去噪，提取主成分降低维度 
* LDA：有监督线性降维，投影后类内方差最小，类间方差最大 
* 联系：都能降维，都假设数据是高斯分布 
* 区别： 
    * PCA只能降维，LDA还能分类 
    * 有无监督，LDA有监督 
    * LDA最多降维到C-1，PCA无限制 
    * LDA选择分类性能最好的投影方向，PCA选择投影方差最大的方向 
* https://blog.csdn.net/sinat_30353259/article/details/81569550
* https://zhuanlan.zhihu.com/p/111631197
* 最好手推一遍  