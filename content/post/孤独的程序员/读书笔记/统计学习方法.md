---
title: 统计学习方法笔记
date: 2020-03-18
draft: true
categories:
- 无聊的幻想家
tags:
- 统计学

---

## 概论

  * 模型、策略、算法
  * 经验风险/结构风险最小化
  * 过拟合
    * 正则化：L1/L2
    * 交叉验证：划分验证集再不同复杂度模型中选择测试误差最小的
  * 生成模型
    * 能还原出P(X, Y)
    * 基本只有朴素贝叶斯、HMM
  * 判别模型
    * 直接学习P(Y|X)
    * SVM等大多数机器学习模型


## 感知机
  * 损失函数：误分类点到超平面的总距离
  * 初值敏感、梯度下降更新参数
  * 线性可分时，迭代收敛：Novikoff定理


## k近邻
  * 三要素
    * 距离函数
    * k值大小：K越大，复杂度越小，交叉验证选择最佳K
    * 分类决策规则：多数表决
  * 实现：kd树


## 朴素贝叶斯法
  * 条件独立假设：分类特征在类确定情况下相互独立
  * 0-1损失期望风险最小化等价于后验概率最大化
  * 概论估计方法
    * 极大似然估计
    * 贝叶斯估计（拉普拉斯平滑）


## 决策树
  * ID3
    * 信息增益
    * 简单快速
  * C4.5
    * 信息增益比：能够避免选择属性值较多的属性
    * 能够处理连续值与缺失值
    * 剪枝：前后的损失函数决定是否剪枝
  * CART
    * Gini指数：纯度、简化运算
    * 分类：Gini指数最小化
    * 回归：平方损失最小化
    * 剪枝：子树序列中交叉验证选择最佳子树
    * 所得树一定为二叉树


## 逻辑回归与最大熵模型
  * 逻辑回归是对数几率线性模型
  * 最大熵原理：所有可能的数据分布概率模型中，熵最大的模型时最好的


## SVM
  * 函数距离与几何距离
  * 完全线性可分：硬间隔
  * 近似线性可分：软间隔
  * 完全非线性可分：核方法
  * 算法：SMO
  * 合页损失
    * 线性支持向量机学习等价于最小化二阶范数正则化的合页函数
    * 合页损失不仅要分类正确，而且确信度足够高时损失才是0


## Boosting
  * AdaBoost：
    * 能适应弱分类器各自的训练误差率
    * 加法模型
    * 损失函数为指数损失
    * 前向分步算法
  * GBDT
    * 回归：平方误差损失函数，每次拟合残差
    * 分类：指数损失函数
    * 一般问题：梯度提升


## EM
  * E步：对对数似然求期望
  * M步：求最大化期望对数似然得到参数更新值
  * 收敛性：对数似然函数序列收敛，不保证收敛到最大值
  * 例子：三硬币、高斯混合模型


## HMM（隐马尔可夫）
  * 概率计算：前向后向算法
  * 学习模型参数：EM算法
  * 预测状态序列：维特比算法


## CRF（条件随机场）
  * 输出变量是无向图
  * 边特征、点特征