<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>从seq2seq到Transformer | Shuryne</title>

<meta name="keywords" content="计算机, NLP, Transformer" />
<meta name="description" content="
关于Seq2Seq和Transformer
">
<meta name="author" content="Shuryne">
<link rel="canonical" href="https://Shuryne.github.io/posts/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/attention-is-all-you-need/" />
<link href="/assets/css/stylesheet.min.599b0e9ad02d274d3cb655359f597ea4ffb4913107f886d7b6869ff79a84a726.css" integrity="sha256-WZsOmtAtJ008tlU1n1l&#43;pP&#43;0kTEH&#43;IbXtoaf95qEpyY=" rel="preload stylesheet"
    as="style">

<link rel="icon" href="https://Shuryne.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://Shuryne.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://Shuryne.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://Shuryne.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://Shuryne.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.81.0" />


<meta property="og:title" content="从seq2seq到Transformer" />
<meta property="og:description" content="
关于Seq2Seq和Transformer
" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Shuryne.github.io/posts/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/attention-is-all-you-need/" />
<meta property="og:image" content="https://Shuryne.github.io/papermod-cover.png"/>
<meta property="article:published_time" content="2018-11-12T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-03-20T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://Shuryne.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="从seq2seq到Transformer"/>
<meta name="twitter:description" content="
关于Seq2Seq和Transformer
"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://Shuryne.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "从seq2seq到Transformer",
      "item": "https://Shuryne.github.io/posts/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/attention-is-all-you-need/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "从seq2seq到Transformer",
  "name": "从seq2seq到Transformer",
  "description": " 关于Seq2Seq和Transformer\n",
  "keywords": [
    "计算机", "NLP", "Transformer"
  ],
  "articleBody": " 关于Seq2Seq和Transformer\n 1. Sequence to Sequence Learning with Neural Networks   encoder以及decoder都采用LSTM模型\n  模型很简单，就是最普通的多层LSTM\n  实现的不同之处：\n 用两种不同的LSTM，一种处理输入序列，一种处理输出序列；以可忽略的计算成本增加参数数量，同时在多个语言对上训练LSTM； 更深的LSTM会比浅的效果更好，故论文模型选择了四层； 将输入的序列翻转之后作为输入效果提升；    decoder应用了beam search来提升效果（每次生成词是取使得整个概率最高的前k个词作为候选）；beam size越大，效果越好，同时计算代价增大；\n  关于倒序输入效果提升：rnn是有偏模型，顺序越靠后的单词在最终占据的信息量越大，正序时最后一个词对应的state作为decoder的输入来预测第一个词，在alignment上来看显然这两个词并不是对齐的；倒序的话，first word成了last word，在last state中占据了主导，来预测decoder的第一个词，从某种意义上说实现了alignment，故效果提升；\n  decoder本质上是一个rnn语言模型，不同的是在生成词的时候依赖于encoder的最后一个hidden state：\n  参考资料：\n Sequence to Sequence Learning with Neural Networks https://zhuanlan.zhihu.com/p/26985192     2. Convolutional Sequence to Sequence Learning   Seq2Seq 先把语句转换为一组词向量，通过对词向量的剪辑，提炼出语义向量。但对于如何剪辑有争议。有人提议使用LSTM等循环模型（RNN），来实现语义剪辑。RNN 模型的剪辑手段是：记忆门、遗忘门、和输出门。\n  RNN 模型的突出优势，是很好地解决了长距离依赖的难题；\n  作者使用CNN取代RNN循环模型，训练速度提高9倍，精度超越RNN；\n  为什么更快？\n 卷积并行处理，而循环只能顺序处理，多个机器同时并行训练卷积模型，速度比串行训练循环模型快很多； 可用GPU芯片来加速卷积模型的训练，而暂时还没有硬件能够加速RNN的训练；    为什么更准？\n CNN的层层抽象，与RNN的三重门，其实异曲同工。虽手段不同，但目的都是忽略次要内容，传承重要内容。所以在精度方面二者差距不大； 但是层级结构与循环网络链结构相比，提供了一种较短的路径来捕获词之间远程的依赖关系，因此也可以更好地捕捉更复杂的关系； Facebook translate 与Google translate的精度差异，应该是由于attention的改进引起的；  Google Translate的解码器使用单层LSTM模型，故attention也是单层的；Facebook的解码器使用CNN模型，是多层的，attention 是多跳的（multi-hop）；越是底层的attention越聚焦，细节越丰富；越是高层的attention，视野越开阔，抽象程度越高，越能抓住文章主旨； Google Translate使用的attention，依赖于编码器生成的语义向量，而不依赖于输入的原生态的词向量。而Facebook的attention，对语义向量和原生态词向量兼收并取；语义向量负责把握主旨，保证解码器的输出不偏题；原生态词向量关注措辞，保障解码器的输出用词得当；      将来attention的机制，还得融入规则。论文里只理解字面意思的attention的计算方式，无法理解“画外音”、“引经据典”、“含沙射影”的联想型语句。要正确理解“引经据典”和“含沙射影”，将来attention机制，还得融入知识图谱（张俊的猜想）；\n  模型结构：encoder-decoder +attention模块\n encoder 和 decoder采用了相同的卷积结构 非线性部分采用门控结构GLM； attention采用多跳注意multi-hop attention，即在decoder的每一个卷积层都会进行attention操作，并将结果输入到下一层；    参考资料\n Convolutional Sequence to Sequence Learning https://zhuanlan.zhihu.com/p/26918935 https://zhuanlan.zhihu.com/p/27464080     3. Attention Is All You Need   优点：\n  靠attention机制，不使用RNN和CNN，并行度高；\n  提出self-attention，自己和自己做attention，使得每个词都有全局的语义信息（长依赖）：由于Self-Attention是每个词和所有词都要计算Attention，所以不管他们中间有多长距离，最大的路径长度也都只是1，可以捕获长距离依赖关系；\n  提出multi-head attention，可以看成attention的ensemble版本，不同head学习不同的子空间语义；\n    参考资料\n Attention Is All You Need https://jalammar.github.io/illustrated-transformer/ https://zhuanlan.zhihu.com/p/39034683   ",
  "wordCount" : "1760",
  "inLanguage": "en",
  "datePublished": "2018-11-12T00:00:00Z",
  "dateModified": "2021-03-20T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Shuryne"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Shuryne.github.io/posts/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/attention-is-all-you-need/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shuryne",
    "logo": {
      "@type": "ImageObject",
      "url": "https://Shuryne.github.io/favicon.ico"
    }
  }
}
</script>





</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://Shuryne.github.io" accesskey="h" title="Shuryne (Alt + H)">Shuryne</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                
                
            </span>
        </div>
        <ul id="menu" onscroll="menu_on_scroll()">
            <li>
                <a href="https://Shuryne.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://Shuryne.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://Shuryne.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://Shuryne.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li></ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">

    <h1 class="post-title">
      从seq2seq到Transformer
    </h1>
    <div class="post-meta">

November 12, 2018&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Shuryne

</div>
  </header> 

  <div class="toc">
    <details  open>
      <summary accesskey="c" title="(Alt + C)">
        <div class="details">Table of Contents</div>
      </summary>
      <div class="inner"><ul><li>
        <a href="#1-sequence-to-sequence-learning-with-neural-networks" aria-label="1. Sequence to Sequence Learning with Neural Networks">1. Sequence to Sequence Learning with Neural Networks</a></li><li>
        <a href="#2-convolutional-sequence-to-sequence-learning" aria-label="2. Convolutional Sequence to Sequence Learning">2. Convolutional Sequence to Sequence Learning</a></li><li>
        <a href="#3-attention-is-all-you-need" aria-label="3. Attention Is All You Need">3. Attention Is All You Need</a></li></ul>
      </div>
    </details>
  </div>
  <div class="post-content">
<blockquote>
<p>关于Seq2Seq和Transformer</p>
</blockquote>
<h4 id="1-sequence-to-sequence-learning-with-neural-networks">1. Sequence to Sequence Learning with Neural Networks<a hidden class="anchor" aria-hidden="true" href="#1-sequence-to-sequence-learning-with-neural-networks">#</a></h4>
<ul>
<li>
<p><code>encoder</code>以及<code>decoder</code>都采用<code>LSTM</code>模型</p>
</li>
<li>
<p>模型很简单，就是最普通的多层<code>LSTM</code></p>
</li>
<li>
<p>实现的不同之处：</p>
<ul>
<li>用两种不同的<code>LSTM</code>，一种处理输入序列，一种处理输出序列；以可忽略的计算成本增加参数数量，同时在多个语言对上训练<code>LSTM</code>；</li>
<li>更深的<code>LSTM</code>会比浅的效果更好，故论文模型选择了四层；</li>
<li>将输入的序列翻转之后作为输入效果提升；</li>
</ul>
</li>
<li>
<p><code>decoder</code>应用了beam search来提升效果（每次生成词是取使得整个概率最高的前k个词作为候选）；beam size越大，效果越好，同时计算代价增大；</p>
</li>
<li>
<p>关于倒序输入效果提升：<code>rnn</code>是有偏模型，顺序越靠后的单词在最终占据的信息量越大，正序时最后一个词对应的state作为<code>decoder</code>的输入来预测第一个词，在<code>alignment</code>上来看显然这两个词并不是对齐的；倒序的话，<code>first word</code>成了<code>last word</code>，在<code>last state</code>中占据了主导，来预测<code>decoder</code>的第一个词，从某种意义上说实现了<code>alignment</code>，故效果提升；</p>
</li>
<li>
<p><code>decoder</code>本质上是一个<code>rnn</code>语言模型，不同的是在生成词的时候依赖于<code>encoder</code>的最后一个<code>hidden state</code>：</p>
</li>
<li>
<p>参考资料：</p>
<ul>
<li>Sequence to Sequence Learning with Neural Networks</li>
<li><a href="https://zhuanlan.zhihu.com/p/26985192">https://zhuanlan.zhihu.com/p/26985192</a></li>
</ul>
</li>
</ul>
<hr>
<h4 id="2-convolutional-sequence-to-sequence-learning">2. Convolutional Sequence to Sequence Learning<a hidden class="anchor" aria-hidden="true" href="#2-convolutional-sequence-to-sequence-learning">#</a></h4>
<ul>
<li>
<p>Seq2Seq 先把语句转换为一组词向量，通过对词向量的剪辑，提炼出语义向量。但对于如何剪辑有争议。有人提议使用<code>LSTM</code>等循环模型（RNN），来实现语义剪辑。RNN 模型的剪辑手段是：记忆门、遗忘门、和输出门。</p>
</li>
<li>
<p>RNN 模型的突出优势，是很好地解决了长距离依赖的难题；</p>
</li>
<li>
<p>作者使用<code>CNN</code>取代<code>RNN</code>循环模型，训练速度提高9倍，精度超越<code>RNN</code>；</p>
</li>
<li>
<p><strong>为什么更快？</strong></p>
<ul>
<li>卷积并行处理，而循环只能顺序处理，多个机器同时并行训练卷积模型，速度比串行训练循环模型快很多；</li>
<li>可用<code>GPU</code>芯片来加速卷积模型的训练，而暂时还没有硬件能够加速<code>RNN</code>的训练；</li>
</ul>
</li>
<li>
<p><strong>为什么更准？</strong></p>
<ul>
<li><code>CNN</code>的层层抽象，与<code>RNN</code>的三重门，其实异曲同工。虽手段不同，但目的都是忽略次要内容，传承重要内容。所以在精度方面二者差距不大；</li>
<li>但是层级结构与循环网络链结构相比，提供了一种较短的路径来捕获词之间远程的依赖关系，因此也可以更好地捕捉更复杂的关系；</li>
<li><code>Facebook translate</code> 与<code>Google translate</code>的精度差异，应该是由于<code>attention</code>的改进引起的；
<ul>
<li><code>Google Translate</code>的解码器使用单层<code>LSTM</code>模型，故<code>attention</code>也是单层的；<code>Facebook</code>的解码器使用<code>CNN</code>模型，是多层的，<code>attention</code> 是多跳的（multi-hop）；越是底层的<code>attention</code>越聚焦，细节越丰富；越是高层的<code>attention</code>，视野越开阔，抽象程度越高，越能抓住文章主旨；</li>
<li><code>Google Translate</code>使用的<code>attention</code>，依赖于编码器生成的语义向量，而不依赖于输入的原生态的词向量。而<code>Facebook</code>的<code>attention</code>，对语义向量和原生态词向量兼收并取；语义向量负责把握主旨，保证解码器的输出不偏题；原生态词向量关注措辞，保障解码器的输出用词得当；</li>
</ul>
</li>
</ul>
</li>
<li>
<p>将来<code>attention</code>的机制，还得融入规则。论文里只理解字面意思的<code>attention</code>的计算方式，无法理解“画外音”、“引经据典”、“含沙射影”的联想型语句。要正确理解“引经据典”和“含沙射影”，将来<code>attention</code>机制，还得融入知识图谱（张俊的猜想）；</p>
</li>
<li>
<p>模型结构：<code>encoder-decoder</code> +<code>attention</code>模块</p>
<ul>
<li>encoder 和 decoder采用了相同的卷积结构</li>
<li>非线性部分采用门控结构<code>GLM</code>；</li>
<li><code>attention</code>采用多跳注意<code>multi-hop attention</code>，即在<code>decoder</code>的每一个卷积层都会进行<code>attention</code>操作，并将结果输入到下一层；</li>
</ul>
</li>
<li>
<p>参考资料</p>
<ul>
<li>Convolutional Sequence to Sequence Learning</li>
<li><a href="https://zhuanlan.zhihu.com/p/26918935">https://zhuanlan.zhihu.com/p/26918935</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/27464080">https://zhuanlan.zhihu.com/p/27464080</a></li>
</ul>
</li>
</ul>
<hr>
<h4 id="3-attention-is-all-you-need">3. Attention Is All You Need<a hidden class="anchor" aria-hidden="true" href="#3-attention-is-all-you-need">#</a></h4>
<ul>
<li>
<p>优点：</p>
<ul>
<li>
<p>靠<code>attention</code>机制，不使用<code>RNN</code>和<code>CNN</code>，并行度高；</p>
</li>
<li>
<p>提出<code>self-attention</code>，自己和自己做<code>attention</code>，使得每个词都有全局的语义信息（长依赖）：由于<code>Self-Attention</code>是每个词和所有词都要计算<code>Attention</code>，所以不管他们中间有多长距离，最大的路径长度也都只是1，可以捕获长距离依赖关系；</p>
</li>
<li>
<p>提出<code>multi-head attention</code>，可以看成<code>attention</code>的<code>ensemble</code>版本，不同<code>head</code>学习不同的子空间语义；</p>
</li>
</ul>
</li>
<li>
<p>参考资料</p>
<ul>
<li>Attention Is All You Need</li>
<li><a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/39034683">https://zhuanlan.zhihu.com/p/39034683</a></li>
</ul>
</li>
</ul>
</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://Shuryne.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/">计算机</a></li>
      <li><a href="https://Shuryne.github.io/tags/nlp/">NLP</a></li>
      <li><a href="https://Shuryne.github.io/tags/transformer/">Transformer</a></li>
    </ul>
    <nav class="paginav">
      <a class="prev" href="https://Shuryne.github.io/posts/%E5%B7%A5%E5%85%B7%E7%AC%94%E8%AE%B0/tmux%E7%AC%94%E8%AE%B0/">
        <span class="title">« Prev Page</span>
        <br>
        <span>提高开发效率：Tmux常用命令</span>
      </a>
      <a class="next" href="https://Shuryne.github.io/posts/%E6%9D%82%E7%9A%84%E6%96%87/%E7%94%B5%E5%95%86%E7%9A%84%E5%A4%A9%E8%8A%B1%E6%9D%BF%E5%9C%A8%E5%93%AA%E9%87%8C/">
        <span class="title">Next Page »</span>
        <br>
        <span>电商的天花板在哪里</span>
      </a>
    </nav>
  </footer>
</article>
    </main><footer class="footer">
    <span>&copy; 2021 <a href="https://Shuryne.github.io">Shuryne</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>



<script defer src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>

</body>

</html>
