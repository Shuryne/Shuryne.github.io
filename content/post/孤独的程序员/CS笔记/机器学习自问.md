---
title: 机器学习自问
date: 2021-06-26
draft: true
categories:
- 孤独的程序员
tags:

---

###### 书籍：李航《统计机器学习 》

## 传统模型

### 1. L1和L2的区别联系是什么？ 

* L1对应拉普拉斯分布，L2对应高斯分布
*  L1产生稀疏特征，便于特征选择
* L2产生更多特征但是接近0
* 为什么用L2不用L1：L2计算方便、L2有唯一解，L1有多解 


### 2. 决策树中ID3、C4.5、CART的区别是什么？为什么有这些区别？ 

* ID3：信息增益 
* C4.5：信息增益比，避免ID3中选择属性值较多的属性 
* CART：gini指数简化计算，一定为二叉树 
* https://blog.csdn.net/songhao22/article/details/77150972 


### 3. LR是什么？怎么优化？ 

* 线性回归一般用于回归不能做分类，LR针对此在线性回归的基础上套了一层sigmoid函数用于平滑分类 
* 二分类loss，梯度下降优化，更新参数值为真实值与预测值之间的差值 
* https://zhuanlan.zhihu.com/p/36670444 


### 4. SVM怎么优化？ 



### 5. 有哪些BOOST方法？说说它们之间的区别？XGBOOST为什么比GBDT好？ 

* Adaboost：强调自适应，不断调整样本权重，加入弱分类器进行boosting 
* GBDT： Gradient Descent + Boosting，反复选择一个指向负梯度方向的函数，对目标函数进行优化。GBDT算法基树采用CART回归树，平方损失函数，新树拟合的目标是上一课树的损失函数的负梯度的值 
* XGBOOST：GBDT只利用了一阶梯度信息，在GBDT基础上对损失函数进行二阶展开，并加入正则项，拟合方向更准，速度更快
* GBDT是一种算法，而XGBoost则是GBDT的系统性工程实现
* XGBOOST相对于GBDT的优势
    * 加入正则，防止过拟合
    * 利用二阶信息，速度更快
    * 不仅支持CART分类器，也支持线性分类器
    * 其他性能方面的加速 
* https://zhuanlan.zhihu.com/p/42740654
* https://www.zhihu.com/question/41354392 


### 6. 集成学习有哪些方法？有什么区别？ 

* Bagging： 
* Boosting： 
* 随机森林：  


### 7. EM算法是什么？HMM和CRF的区别是什么？一般用于什么场景？ 



### 8. ROC曲线是什么？AUC表示什么含义？ 

* ROC曲线为TP-FN图 
* AUC为ROC下的面积，表示分类器的分类能力，同时考虑对于正/负例的分类能力，能够规避样本不平衡带来的问题 
* https://www.zhihu.com/question/39840928
* https://blog.csdn.net/u013385925/article/details/80385873 



### 9. PCA怎么操作？LDA怎么操作？两者之间有什么区别和联系？ 

* PCA：无监督线性降维，去噪，提取主成分降低维度 
* LDA：有监督线性降维，投影后类内方差最小，类间方差最大 
* 联系：都能降维，都假设数据是高斯分布 
* 区别： 
    * PCA只能降维，LDA还能分类 
    * 有无监督，LDA有监督 
    * LDA最多降维到C-1，PCA无限制 
    * LDA选择分类性能最好的投影方向，PCA选择投影方差最大的方向 
* https://blog.csdn.net/sinat_30353259/article/details/81569550
* https://zhuanlan.zhihu.com/p/111631197
* 最好手推一遍  


## 深度模型

### 1. 怎么理解卷积核？ 



### 2.RNN有什么优缺点？为什么会梯度消失/爆炸？LSTM怎么解决梯度消失/爆炸？ 

* RNN梯度消失/爆炸 
    * https://zhuanlan.zhihu.com/p/28687529
    * https://zhuanlan.zhihu.com/p/199389873
* LSTM 
    * https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html#fn:3
    * https://www.cnblogs.com/bonelee/p/10475453.html


### 3. 怎么避免神经网络陷入局部最优？ 

* 多个初始化 
* SGD 
* 模拟退火：以一定概率接受较差值  

