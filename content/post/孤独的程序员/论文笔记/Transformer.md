---
title: NLP基石：Transformer
date: 2018-11-12
draft: false
categories:
- 孤独的程序员
tags:
- attention
- transformer

---

2017年底，准确的说是11月，是计算机人工智能领域一个重要的时间节点。因为在那个时候，论文《[Attention Is All You Need][1]》发布了。Transformer的发布，为NLP领域新的研究范式（预训练+微调）奠定了基础，也成为了人工智能众多领域的一大基石，成千上万的前沿研究或多或少都能在浮光掠影中发现它的身影。

这篇论文也是我入坑NLP以来阅读和翻阅次数最多的论文，无论是阅读Transformer-based的相关论文和文章、魔改模型、算法面试、还是遇到Attention相关的问题，总会回到这篇经典论文。

<!--more-->

## 论文介绍

### Attention



### Self-Attention

捕获长距离依赖，任意两个词之间的路径长度都为1

### Multi-Head Attention

不同的head学习不同子空间，attention的ensemble版本





[1]: https://arxiv.org/pdf/1706.03762.pdf "Attention is all you need"
[2]: https://jalammar.github.io/illustrated-transformer/
